# üöÄ Guia Completo de Self-Hosting do EditAI com DeepSeek Local

Este guia detalhado mostrar√° como hospedar completamente o EditAI fora do Lovable e processar tudo localmente com DeepSeek via Ollama.

---

## üìë √çndice

1. [Vis√£o Geral da Arquitetura](#vis√£o-geral-da-arquitetura)
2. [Pr√©-requisitos](#pr√©-requisitos)
3. [Parte 1: Configurando Ollama e DeepSeek](#parte-1-configurando-ollama-e-deepseek)
4. [Parte 2: Configurando o Backend Node.js](#parte-2-configurando-o-backend-nodejs)
5. [Parte 3: Configurando o Frontend](#parte-3-configurando-o-frontend)
6. [Parte 4: Testando Localmente](#parte-4-testando-localmente)
7. [Parte 5: Deploy em Produ√ß√£o](#parte-5-deploy-em-produ√ß√£o)
8. [Troubleshooting](#troubleshooting)
9. [Otimiza√ß√µes e Performance](#otimiza√ß√µes-e-performance)
10. [Seguran√ßa](#seguran√ßa)

---

## üèóÔ∏è Vis√£o Geral da Arquitetura

### Antes (Lovable Cloud)
```
Frontend (React) ‚Üí Supabase Edge Functions ‚Üí Lovable AI Gateway ‚Üí Gemini
```

### Depois (Self-Hosted)
```
Frontend (React) ‚Üí Backend Node.js ‚Üí Ollama ‚Üí DeepSeek (Local)
```

**Componentes:**
- **Frontend**: Aplica√ß√£o React est√°tica (pode ser hospedada em Netlify, Vercel, servidor pr√≥prio)
- **Backend**: Servidor Node.js/Express (precisa rodar em servidor com acesso ao Ollama)
- **Ollama**: Runtime para executar LLMs localmente
- **DeepSeek**: Modelo de IA executado via Ollama

---

## üìã Pr√©-requisitos

### Hardware Recomendado

Para rodar DeepSeek localmente, voc√™ precisar√° de:

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| **RAM** | 16 GB | 32 GB+ |
| **GPU** | CPU only (lento) | NVIDIA GPU 8GB+ VRAM |
| **Armazenamento** | 20 GB livre | 50 GB+ SSD |
| **CPU** | 4 cores | 8+ cores |

**Modelos DeepSeek e Requisitos:**
- `deepseek-r1:1.5b` - 2 GB RAM - CPU ok
- `deepseek-r1:7b` - 8 GB RAM - R√°pido
- `deepseek-r1:8b` - 8 GB RAM - Balanceado (Recomendado)
- `deepseek-r1:14b` - 16 GB RAM - Melhor qualidade
- `deepseek-r1:32b` - 32 GB RAM - Alta qualidade
- `deepseek-r1:70b` - 64 GB RAM - M√°xima qualidade

### Software Necess√°rio

1. **Sistema Operacional**
   - Linux (Ubuntu 20.04+, Debian, etc.) - Recomendado
   - macOS 12+ (Monterey ou superior)
   - Windows 10/11 com WSL2

2. **Node.js**
   - Vers√£o 18.x ou superior
   - npm 9.x ou superior
   - [Download](https://nodejs.org/)

3. **Git**
   - Para clonar o reposit√≥rio
   - [Download](https://git-scm.com/)

4. **Ollama**
   - Runtime para executar LLMs
   - [Download](https://ollama.ai/)

---

## üîß Parte 1: Configurando Ollama e DeepSeek

### Passo 1.1: Instalar Ollama

#### No Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### No macOS:
```bash
# Baixe o instalador em https://ollama.ai/download/mac
# Ou use Homebrew:
brew install ollama
```

#### No Windows:
```bash
# Baixe o instalador em https://ollama.ai/download/windows
# Ou use WSL2 e siga as instru√ß√µes do Linux
```

### Passo 1.2: Verificar Instala√ß√£o do Ollama

```bash
# Verifique se Ollama foi instalado corretamente
ollama --version

# Deve mostrar algo como: ollama version 0.x.x
```

### Passo 1.3: Iniciar o Servidor Ollama

```bash
# Inicie o servidor Ollama (rodar√° em http://localhost:11434)
ollama serve
```

**Nota:** Deixe este terminal aberto! O Ollama precisa estar rodando sempre.

**Configurar como Servi√ßo (Opcional - Linux):**
```bash
# Criar arquivo de servi√ßo systemd
sudo nano /etc/systemd/system/ollama.service
```

Adicione este conte√∫do:
```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=seu-usuario
ExecStart=/usr/local/bin/ollama serve
Restart=always
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=multi-user.target
```

```bash
# Ative e inicie o servi√ßo
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

### Passo 1.4: Baixar o Modelo DeepSeek

Abra um **novo terminal** e execute:

```bash
# Modelo recomendado (8GB RAM, boa performance)
ollama pull deepseek-r1:8b

# OU outros modelos conforme seu hardware:
# ollama pull deepseek-r1:1.5b  # Mais leve (2GB)
# ollama pull deepseek-r1:14b   # Melhor qualidade (16GB)
# ollama pull deepseek-r1:32b   # Alta qualidade (32GB)
```

**Nota:** O download pode levar v√°rios minutos dependendo da sua conex√£o!

### Passo 1.5: Testar o Modelo

```bash
# Teste se o modelo est√° funcionando
ollama run deepseek-r1:8b "Ol√°, voc√™ pode me ajudar com editais p√∫blicos?"

# Voc√™ deve receber uma resposta em texto
```

### Passo 1.6: Listar Modelos Instalados

```bash
# Veja todos os modelos instalados
ollama list

# Sa√≠da esperada:
# NAME                    ID              SIZE      MODIFIED
# deepseek-r1:8b          abc123...       4.7 GB    2 hours ago
```

### Passo 1.7: Verificar API do Ollama

```bash
# Teste a API REST do Ollama
curl http://localhost:11434/api/tags

# Deve retornar um JSON com os modelos instalados
```

‚úÖ **Checkpoint 1:** Se voc√™ conseguiu ver os modelos listados, o Ollama est√° funcionando corretamente!

---

## üñ•Ô∏è Parte 2: Configurando o Backend Node.js

### Passo 2.1: Clonar o Reposit√≥rio

```bash
# Clone o reposit√≥rio do GitHub (se ainda n√£o clonou)
git clone <URL_DO_SEU_REPOSITORIO>
cd editai
```

### Passo 2.2: Navegar at√© o Backend

```bash
cd backend
```

### Passo 2.3: Instalar Depend√™ncias

```bash
# Instale as depend√™ncias do Node.js
npm install

# Deve instalar:
# - express (servidor web)
# - cors (permitir chamadas do frontend)
# - axios (fazer chamadas HTTP ao Ollama)
# - dotenv (gerenciar vari√°veis de ambiente)
```

### Passo 2.4: Configurar Vari√°veis de Ambiente

```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Abra o arquivo .env para editar
nano .env  # ou use seu editor preferido (code .env, vim .env, etc.)
```

**Configure o arquivo `.env`:**

```env
# Porta do servidor backend
PORT=3001

# URL base do Ollama (geralmente localhost se estiver na mesma m√°quina)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo DeepSeek que voc√™ baixou
OLLAMA_MODEL=deepseek-r1:8b

# Origens permitidas (adicione a URL do seu frontend)
# Para desenvolvimento local:
ALLOWED_ORIGINS=http://localhost:8080,http://localhost:5173,http://localhost:3000

# Para produ√ß√£o, adicione seu dom√≠nio:
# ALLOWED_ORIGINS=https://seu-dominio.com,http://localhost:8080

# Ambiente
NODE_ENV=development
```

**Explica√ß√£o das Vari√°veis:**

- `PORT`: Porta onde o backend rodar√° (padr√£o: 3001)
- `OLLAMA_BASE_URL`: URL do servidor Ollama
  - Se Ollama estiver na mesma m√°quina: `http://localhost:11434`
  - Se estiver em outra m√°quina: `http://IP_DA_MAQUINA:11434`
  - Se estiver em container Docker: `http://ollama:11434`
- `OLLAMA_MODEL`: Nome exato do modelo (use `ollama list` para ver os nomes)
- `ALLOWED_ORIGINS`: URLs do frontend que podem fazer requisi√ß√µes (separadas por v√≠rgula)
- `NODE_ENV`: `development` para dev, `production` para produ√ß√£o

### Passo 2.5: Testar o Backend

```bash
# Inicie o servidor backend em modo desenvolvimento
npm run dev

# Voc√™ deve ver:
# ‚úÖ EditAI Backend running on port 3001
# üì° Ollama: http://localhost:11434
# ü§ñ Model: deepseek-r1:8b
# üåê CORS enabled for: http://localhost:8080
```

### Passo 2.6: Testar Endpoints da API

Abra um **novo terminal** e teste:

```bash
# Teste o health check
curl http://localhost:3001/health

# Resposta esperada:
# {"status":"ok","ollama":"http://localhost:11434","model":"deepseek-r1:8b"}
```

```bash
# Teste o endpoint de sugest√£o de projeto
curl -X POST http://localhost:3001/api/suggest-project \
  -H "Content-Type: application/json" \
  -d '{"editalUrl":"https://exemplo.com/edital-teste"}'

# Deve retornar um JSON com a sugest√£o (pode demorar alguns segundos)
```

‚úÖ **Checkpoint 2:** Se o health check funcionou e o backend est√° rodando, est√° tudo certo!

---

## üíª Parte 3: Configurando o Frontend

### Passo 3.1: Voltar para a Raiz do Projeto

```bash
# Se estiver na pasta backend, volte para a raiz
cd ..
```

### Passo 3.2: Instalar Depend√™ncias do Frontend

```bash
# Instale todas as depend√™ncias
npm install
```

### Passo 3.3: Configurar Vari√°vel de Ambiente do Frontend

```bash
# Copie o arquivo de exemplo
cp .env.local.example .env.local

# Abra para editar
nano .env.local
```

**Configure o arquivo `.env.local`:**

```env
# URL do backend Node.js
# Para desenvolvimento local:
VITE_BACKEND_URL=http://localhost:3001

# Para produ√ß√£o (substitua pelo seu dom√≠nio):
# VITE_BACKEND_URL=https://api.seu-dominio.com
```

**‚ö†Ô∏è IMPORTANTE:** 
- Vari√°veis do Vite DEVEM come√ßar com `VITE_`
- Ap√≥s alterar `.env.local`, voc√™ precisa reiniciar o servidor de desenvolvimento

### Passo 3.4: Verificar C√≥digo do Frontend

O c√≥digo j√° foi modificado para usar o backend local ao inv√©s do Supabase. Verifique se est√° correto:

```bash
# Verifique se os componentes est√£o usando fetch ao inv√©s de supabase
grep -r "VITE_BACKEND_URL" src/components/
```

Deve retornar:
- `src/components/EditalForm.tsx`
- `src/components/ProjectSuggestionForm.tsx`
- `src/components/ProjectSuggestionEditor.tsx`

---

## üß™ Parte 4: Testando Localmente

### Passo 4.1: Iniciar Todos os Servi√ßos

Voc√™ precisar√° de **3 terminais** abertos:

**Terminal 1 - Ollama:**
```bash
# Se n√£o estiver rodando como servi√ßo
ollama serve
```

**Terminal 2 - Backend:**
```bash
cd backend
npm run dev

# Deve mostrar: ‚úÖ EditAI Backend running on port 3001
```

**Terminal 3 - Frontend:**
```bash
# Na raiz do projeto
npm run dev

# Deve mostrar algo como:
# VITE v5.x.x  ready in xxx ms
# ‚ûú  Local:   http://localhost:8080/
```

### Passo 4.2: Acessar a Aplica√ß√£o

Abra seu navegador e acesse:
```
http://localhost:8080
```

### Passo 4.3: Testar as Funcionalidades

#### Teste 1: Sugest√£o de Projeto

1. Na p√°gina inicial, clique em **"Precisa de um Projeto?"**
2. Cole uma URL de teste: `https://exemplo.gov.br/edital-teste`
3. Clique em **"Gerar Sugest√£o de Projeto"**
4. Aguarde (pode demorar 30-60 segundos na primeira vez)
5. Voc√™ deve ver os campos preenchidos com a sugest√£o do DeepSeek

#### Teste 2: Edi√ß√£o e PDF

1. Edite qualquer campo da sugest√£o
2. Clique em **"Gerar PDF da Proposta"**
3. Um PDF deve ser baixado automaticamente

### Passo 4.4: Monitorar Logs

Observe os logs nos terminais:

**Backend (Terminal 2):**
```
Processing suggest-project for: https://exemplo.gov.br/edital-teste
POST /api/suggest-project 200 - 45.678 ms
```

**Ollama (Terminal 1):**
Voc√™ ver√° atividade quando o modelo estiver processando.

### Passo 4.5: Verificar DevTools do Navegador

1. Pressione `F12` para abrir DevTools
2. V√° na aba **Network**
3. Fa√ßa uma requisi√ß√£o
4. Verifique:
   - Requisi√ß√£o vai para `http://localhost:3001`
   - Status √© `200 OK`
   - Response tem os dados esperados

‚úÖ **Checkpoint 3:** Se conseguiu gerar uma sugest√£o e baixar o PDF, tudo est√° funcionando!

---

## üåê Parte 5: Deploy em Produ√ß√£o

### Op√ß√£o A: Servidor VPS √önico (Recomendado para Iniciantes)

Use um VPS (DigitalOcean, Linode, AWS EC2, etc.) que rode tudo:

#### Requisitos do Servidor:
- Ubuntu 22.04 LTS
- 16 GB RAM (m√≠nimo) - 32 GB (recomendado)
- 4 vCPUs (m√≠nimo) - 8 vCPUs (recomendado)
- 50 GB SSD
- GPU opcional (melhora muito a performance)

#### Passo A.1: Conectar ao Servidor

```bash
ssh usuario@seu-servidor.com
```

#### Passo A.2: Instalar Depend√™ncias

```bash
# Atualizar sistema
sudo apt update && sudo apt upgrade -y

# Instalar Node.js 18
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt install -y nodejs

# Instalar Git
sudo apt install -y git

# Instalar Nginx (servidor web)
sudo apt install -y nginx

# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Passo A.3: Baixar Modelo DeepSeek

```bash
ollama pull deepseek-r1:8b
```

#### Passo A.4: Clonar e Configurar o Projeto

```bash
# Criar diret√≥rio para o projeto
mkdir -p /var/www/editai
cd /var/www/editai

# Clonar reposit√≥rio
git clone <URL_DO_REPOSITORIO> .

# Instalar depend√™ncias do backend
cd backend
npm install --production

# Configurar .env
cp .env.example .env
nano .env
```

**Configure `.env` para produ√ß√£o:**
```env
PORT=3001
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1:8b
ALLOWED_ORIGINS=https://seu-dominio.com
NODE_ENV=production
```

#### Passo A.5: Configurar PM2 (Process Manager)

```bash
# Instalar PM2 globalmente
sudo npm install -g pm2

# Iniciar backend com PM2
cd /var/www/editai/backend
pm2 start server.js --name editai-backend

# Salvar configura√ß√£o
pm2 save

# Configurar PM2 para iniciar no boot
pm2 startup
# Copie e execute o comando que aparecer
```

#### Passo A.6: Configurar Ollama como Servi√ßo

```bash
# Criar arquivo de servi√ßo
sudo nano /etc/systemd/system/ollama.service
```

Adicione:
```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=seu-usuario
ExecStart=/usr/local/bin/ollama serve
Restart=always
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=multi-user.target
```

```bash
# Ativar e iniciar
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### Passo A.7: Build do Frontend

```bash
cd /var/www/editai

# Configurar .env.local para produ√ß√£o
nano .env.local
```

Adicione:
```env
VITE_BACKEND_URL=https://api.seu-dominio.com
```

```bash
# Build do frontend
npm install
npm run build

# Os arquivos estar√£o em dist/
```

#### Passo A.8: Configurar Nginx

```bash
# Criar configura√ß√£o do site
sudo nano /etc/nginx/sites-available/editai
```

Adicione:
```nginx
# Frontend
server {
    listen 80;
    server_name seu-dominio.com www.seu-dominio.com;
    
    root /var/www/editai/dist;
    index index.html;
    
    location / {
        try_files $uri $uri/ /index.html;
    }
    
    # Seguran√ßa
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
}

# Backend API
server {
    listen 80;
    server_name api.seu-dominio.com;
    
    location / {
        proxy_pass http://localhost:3001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeout maior para LLM
        proxy_read_timeout 300s;
        proxy_connect_timeout 300s;
    }
}
```

```bash
# Ativar configura√ß√£o
sudo ln -s /etc/nginx/sites-available/editai /etc/nginx/sites-enabled/

# Testar configura√ß√£o
sudo nginx -t

# Reiniciar Nginx
sudo systemctl restart nginx
```

#### Passo A.9: Configurar SSL com Let's Encrypt

```bash
# Instalar Certbot
sudo apt install -y certbot python3-certbot-nginx

# Obter certificados SSL
sudo certbot --nginx -d seu-dominio.com -d www.seu-dominio.com -d api.seu-dominio.com

# Renova√ß√£o autom√°tica j√° est√° configurada
```

#### Passo A.10: Configurar Firewall

```bash
# Configurar UFW
sudo ufw allow 22/tcp   # SSH
sudo ufw allow 80/tcp   # HTTP
sudo ufw allow 443/tcp  # HTTPS
sudo ufw enable
```

‚úÖ **Deploy Completo!** Acesse `https://seu-dominio.com`

---

### Op√ß√£o B: Frontend e Backend Separados

#### Frontend: Netlify/Vercel (Gr√°tis)

**Build Settings (Netlify/Vercel):**
- **Build command:** `npm run build`
- **Publish directory:** `dist`
- **Environment variables:**
  - `VITE_BACKEND_URL=https://api.seu-dominio.com`

#### Backend: VPS com Ollama

Siga os passos A.2 a A.6 da Op√ß√£o A, mas apenas para o backend.

---

### Op√ß√£o C: Docker Compose (Avan√ßado)

Crie `docker-compose.yml` na raiz:

```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=deepseek-r1:8b
      - PORT=3001
    depends_on:
      - ollama
    restart: unless-stopped
  
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
  
  frontend:
    build: .
    ports:
      - "80:80"
    environment:
      - VITE_BACKEND_URL=http://backend:3001
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  ollama_data:
```

```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Baixar modelo DeepSeek
docker exec -it <ollama-container-id> ollama pull deepseek-r1:8b
```

---

## üîç Troubleshooting

### Problema 1: Ollama n√£o inicia

**Sintoma:** `curl: (7) Failed to connect to localhost port 11434`

**Solu√ß√£o:**
```bash
# Verificar se est√° rodando
ps aux | grep ollama

# Verificar logs
journalctl -u ollama -f

# Reiniciar
sudo systemctl restart ollama
```

### Problema 2: Modelo n√£o encontrado

**Sintoma:** `Error: model 'deepseek-r1:8b' not found`

**Solu√ß√£o:**
```bash
# Listar modelos instalados
ollama list

# Se n√£o estiver, baixe
ollama pull deepseek-r1:8b

# Verifique o nome exato e atualize .env
```

### Problema 3: Backend n√£o conecta ao Ollama

**Sintoma:** `Ollama communication failed: connect ECONNREFUSED`

**Solu√ß√£o:**
```bash
# Verificar se Ollama est√° acess√≠vel
curl http://localhost:11434/api/tags

# Se n√£o funcionar, verifique firewall
sudo ufw status

# Verificar se a porta est√° aberta
netstat -tlnp | grep 11434
```

### Problema 4: CORS Error no Frontend

**Sintoma:** `Access to fetch at 'http://localhost:3001' from origin 'http://localhost:8080' has been blocked by CORS`

**Solu√ß√£o:**
```bash
# Edite backend/.env
nano backend/.env

# Adicione a origem do frontend
ALLOWED_ORIGINS=http://localhost:8080,http://localhost:5173

# Reinicie o backend
pm2 restart editai-backend
```

### Problema 5: Resposta Muito Lenta

**Sintoma:** Requisi√ß√µes demoram mais de 60 segundos

**Solu√ß√µes:**
1. Use um modelo menor: `deepseek-r1:1.5b`
2. Use GPU se dispon√≠vel
3. Aumente RAM dispon√≠vel
4. Reduza o tamanho do prompt
5. Configure context window menor

### Problema 6: Out of Memory

**Sintoma:** `FATAL ERROR: Reached heap limit Allocation failed`

**Solu√ß√£o:**
```bash
# Use modelo menor
ollama pull deepseek-r1:1.5b

# Ou aumente a mem√≥ria do Node.js
NODE_OPTIONS="--max-old-space-size=4096" npm start
```

### Problema 7: Frontend n√£o encontra Backend

**Sintoma:** `Failed to fetch` ou `Network Error`

**Solu√ß√£o:**
```bash
# Verifique se .env.local est√° correto
cat .env.local

# Deve mostrar:
# VITE_BACKEND_URL=http://localhost:3001

# Reinicie o dev server ap√≥s mudar .env.local
npm run dev
```

---

## ‚ö° Otimiza√ß√µes e Performance

### 1. Usar GPU

Se voc√™ tem GPU NVIDIA:

```bash
# Instalar CUDA Toolkit
# Visite: https://developer.nvidia.com/cuda-downloads

# Instalar drivers NVIDIA
sudo ubuntu-drivers autoinstall

# Verificar
nvidia-smi

# Ollama usar√° GPU automaticamente
```

### 2. Configurar Context Window

No backend, edite `server.js`:

```javascript
const requestBody = {
  model: OLLAMA_MODEL,
  messages,
  stream: false,
  format: 'json',
  options: {
    num_ctx: 4096,      // Context window (menor = mais r√°pido)
    temperature: 0.7,    // Criatividade
    top_p: 0.9,         // Nucleus sampling
    num_predict: 2048   // M√°ximo de tokens na resposta
  }
};
```

### 3. Cache de Respostas

Adicione cache Redis para respostas comuns:

```bash
npm install redis
```

### 4. Load Balancing

Para produ√ß√£o com alto tr√°fego, use m√∫ltiplas inst√¢ncias Ollama:

```nginx
upstream ollama_backend {
    server localhost:11434;
    server localhost:11435;
    server localhost:11436;
}
```

### 5. Monitoramento

```bash
# Instalar ferramentas de monitoramento
npm install -g pm2
pm2 install pm2-logrotate

# Monitorar uso de recursos
pm2 monit
```

---

## üîí Seguran√ßa

### 1. Rate Limiting

Adicione ao `backend/server.js`:

```javascript
import rateLimit from 'express-rate-limit';

const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutos
  max: 100 // limite de requests
});

app.use('/api/', limiter);
```

### 2. Helmet (Security Headers)

```bash
cd backend
npm install helmet
```

```javascript
import helmet from 'helmet';
app.use(helmet());
```

### 3. Valida√ß√£o de Input

```javascript
import validator from 'validator';

app.post('/api/suggest-project', (req, res) => {
  const { editalUrl } = req.body;
  
  if (!validator.isURL(editalUrl)) {
    return res.status(400).json({ error: 'Invalid URL' });
  }
  
  // ... resto do c√≥digo
});
```

### 4. HTTPS Obrigat√≥rio

```nginx
# Redirecionar HTTP para HTTPS
server {
    listen 80;
    server_name seu-dominio.com;
    return 301 https://$server_name$request_uri;
}
```

### 5. Limitar Acesso ao Ollama

```bash
# Ollama n√£o deve estar acess√≠vel publicamente
# Certifique-se que apenas localhost pode acessar
sudo ufw deny 11434
```

---

## üìä Monitoramento de Uso

### Logs do Backend

```bash
# Ver logs em tempo real
pm2 logs editai-backend

# Ver logs do Ollama
journalctl -u ollama -f
```

### M√©tricas de Performance

Adicione ao `backend/server.js`:

```javascript
let requestCount = 0;
let totalResponseTime = 0;

app.use((req, res, next) => {
  const start = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - start;
    requestCount++;
    totalResponseTime += duration;
    console.log(`Request ${requestCount}: ${duration}ms`);
  });
  next();
});

// Endpoint de m√©tricas
app.get('/metrics', (req, res) => {
  res.json({
    requests: requestCount,
    avgResponseTime: totalResponseTime / requestCount
  });
});
```

---

## üéØ Checklist Final

- [ ] Ollama instalado e rodando
- [ ] Modelo DeepSeek baixado
- [ ] Backend configurado e rodando
- [ ] Frontend configurado
- [ ] Testado localmente
- [ ] SSL configurado (produ√ß√£o)
- [ ] Firewall configurado
- [ ] Backup configurado
- [ ] Monitoramento ativo
- [ ] Documenta√ß√£o atualizada

---

## üìö Recursos Adicionais

- [Documenta√ß√£o Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [DeepSeek Models](https://github.com/deepseek-ai)
- [Express.js Docs](https://expressjs.com/)
- [Nginx Docs](https://nginx.org/en/docs/)
- [PM2 Docs](https://pm2.keymetrics.io/)

---

## üí° Dicas Finais

1. **Comece Pequeno:** Teste localmente antes de fazer deploy
2. **Monitore Recursos:** DeepSeek consome bastante RAM/GPU
3. **Use Modelo Adequado:** Escolha baseado no seu hardware
4. **Backup Regular:** Sempre fa√ßa backup das configura√ß√µes
5. **Atualize Regularmente:** Mantenha Ollama e depend√™ncias atualizados

---

## üÜò Suporte

Se encontrar problemas:

1. Verifique os logs: `pm2 logs` e `journalctl -u ollama -f`
2. Consulte a se√ß√£o [Troubleshooting](#troubleshooting)
3. Abra uma issue no GitHub
4. Contate suporte@editai.com.br

---

**Desenvolvido com ‚ù§Ô∏è para rodar 100% local e privado**
